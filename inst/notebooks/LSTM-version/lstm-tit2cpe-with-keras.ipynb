{"cells":[{"cell_type":"code","execution_count":1,"metadata":{"_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","trusted":true},"outputs":[{"name":"stdout","output_type":"stream","text":["['annotation', 'inventory.csv', 'lstm_version_trainset.csv', 'ner_trainsets', 'old', 'v_GS_ADD_REMOVE_PROGRAMS.txt']\n"]}],"source":["import os, sys, time, random\n","import h5py\n","import numpy as np\n","print(os.listdir(\"C:/DEVEL/code/data/\"))"]},{"cell_type":"code","execution_count":2,"metadata":{"_uuid":"31ae4ebe3257108c42bdf450073b8d54936a1cd6","trusted":true},"outputs":[],"source":["from keras.layers import *\n","from keras.models import *\n","from keras.utils import *\n","from keras.initializers import *\n","import tensorflow as tf"]},{"cell_type":"code","execution_count":3,"metadata":{"_uuid":"25a4e464236ee5b75cfa6808d8f8ef1f5abc4a5c","trusted":true},"outputs":[],"source":["batch_size = 64  # Batch size for training.\n","epochs = 100  # Number of epochs to train for.\n","latent_dim = 128  # Latent dimensionality of the encoding space.\n","num_samples = 8000  # Number of samples to train on.\n","# Path to the data txt file on disk.\n","data_path = \"C:/DEVEL/code/data/lstm_version_trainset.csv\""]},{"cell_type":"code","execution_count":4,"metadata":{"_uuid":"fa696e274d7302ffd7a7b74cc40ec8a0eac6e60d","trusted":true},"outputs":[],"source":["# Vectorize the data.\n","input_texts = []\n","target_texts = []\n","input_characters = set()\n","target_characters = set()\n","with open(data_path, 'r', encoding='utf-8') as f:\n","    lines = f.read().split('\\n')\n","for line in lines[: min(num_samples, len(lines) - 1)]:\n","    input_text, target_text, extra_text = line.split('\\t')\n","    # We use \"tab\" as the \"start sequence\" character\n","    # for the targets, and \"\\n\" as \"end sequence\" character.\n","    target_text = '\\t' + target_text + '\\n'\n","    input_texts.append(input_text)\n","    target_texts.append(target_text)\n","    for char in input_text:\n","        if char not in input_characters:\n","            input_characters.add(char)\n","    for char in target_text:\n","        if char not in target_characters:\n","            target_characters.add(char)\n","\n","input_characters = sorted(list(input_characters))\n","target_characters = sorted(list(target_characters))\n","num_encoder_tokens = len(input_characters)\n","num_decoder_tokens = len(target_characters)\n","max_encoder_seq_length = max([len(txt) for txt in input_texts])\n","max_decoder_seq_length = max([len(txt) for txt in target_texts])"]},{"cell_type":"code","execution_count":5,"metadata":{"_uuid":"8a616775660a99cbd15478d757edd0f1e918ff23","trusted":true},"outputs":[{"name":"stdout","output_type":"stream","text":["Number of samples: 8000\n","Number of unique input tokens: 76\n","Number of unique output tokens: 38\n","Max sequence length for inputs: 182\n","Max sequence length for outputs: 21\n"]}],"source":["print('Number of samples:', len(input_texts))\n","print('Number of unique input tokens:', num_encoder_tokens)\n","print('Number of unique output tokens:', num_decoder_tokens)\n","print('Max sequence length for inputs:', max_encoder_seq_length)\n","print('Max sequence length for outputs:', max_decoder_seq_length)"]},{"cell_type":"code","execution_count":6,"metadata":{"_uuid":"5392311afe30b92f780c230ac5e681d6076bca02","trusted":true},"outputs":[],"source":["input_token_index = dict(\n","    [(char, i) for i, char in enumerate(input_characters)])\n","target_token_index = dict(\n","    [(char, i) for i, char in enumerate(target_characters)])\n","\n","encoder_input_data = np.zeros(\n","    (len(input_texts), max_encoder_seq_length, num_encoder_tokens),\n","    dtype='float32')\n","decoder_input_data = np.zeros(\n","    (len(input_texts), max_decoder_seq_length, num_decoder_tokens),\n","    dtype='float32')\n","decoder_target_data = np.zeros(\n","    (len(input_texts), max_decoder_seq_length, num_decoder_tokens),\n","    dtype='float32')\n","\n","for i, (input_text, target_text) in enumerate(zip(input_texts, target_texts)):\n","    for t, char in enumerate(input_text):\n","        encoder_input_data[i, t, input_token_index[char]] = 1.\n","    for t, char in enumerate(target_text):\n","        # decoder_target_data is ahead of decoder_input_data by one timestep\n","        decoder_input_data[i, t, target_token_index[char]] = 1.\n","        if t > 0:\n","            # decoder_target_data will be ahead by one timestep\n","            # and will not include the start character.\n","            decoder_target_data[i, t - 1, target_token_index[char]] = 1.\n"]},{"cell_type":"code","execution_count":7,"metadata":{"_uuid":"ecb42239d4b87c222b7513a2ff36fa1651666189","trusted":true},"outputs":[],"source":["# Define an input sequence and process it.\n","encoder_inputs = Input(shape=(None, num_encoder_tokens))\n","encoder = LSTM(latent_dim, return_state=True)\n","encoder_outputs, state_h, state_c = encoder(encoder_inputs)\n","# We discard `encoder_outputs` and only keep the states.\n","encoder_states = [state_h, state_c]\n","\n","# Set up the decoder, using `encoder_states` as initial state.\n","decoder_inputs = Input(shape=(None, num_decoder_tokens))\n","# We set up our decoder to return full output sequences,\n","# and to return internal states as well. We don't use the\n","# return states in the training model, but we will use them in inference.\n","decoder_lstm = LSTM(latent_dim, return_sequences=True, return_state=True)\n","decoder_outputs, _, _ = decoder_lstm(decoder_inputs,\n","                                     initial_state=encoder_states)\n","decoder_dense = Dense(num_decoder_tokens, activation='softmax')\n","decoder_outputs = decoder_dense(decoder_outputs)\n","# Define the model that will turn\n","# `encoder_input_data` & `decoder_input_data` into `decoder_target_data`\n","model = Model([encoder_inputs, decoder_inputs], decoder_outputs)"]},{"cell_type":"code","execution_count":8,"metadata":{"_uuid":"c220e0f83c25b61bcdfcb067eeaa5146d40e956b","trusted":true},"outputs":[{"name":"stdout","output_type":"stream","text":["Model: \"model\"\n","__________________________________________________________________________________________________\n"," Layer (type)                   Output Shape         Param #     Connected to                     \n","==================================================================================================\n"," input_1 (InputLayer)           [(None, None, 76)]   0           []                               \n","                                                                                                  \n"," input_2 (InputLayer)           [(None, None, 38)]   0           []                               \n","                                                                                                  \n"," lstm (LSTM)                    [(None, 128),        104960      ['input_1[0][0]']                \n","                                 (None, 128),                                                     \n","                                 (None, 128)]                                                     \n","                                                                                                  \n"," lstm_1 (LSTM)                  [(None, None, 128),  85504       ['input_2[0][0]',                \n","                                 (None, 128),                     'lstm[0][1]',                   \n","                                 (None, 128)]                     'lstm[0][2]']                   \n","                                                                                                  \n"," dense (Dense)                  (None, None, 38)     4902        ['lstm_1[0][0]']                 \n","                                                                                                  \n","==================================================================================================\n","Total params: 195,366\n","Trainable params: 195,366\n","Non-trainable params: 0\n","__________________________________________________________________________________________________\n","[<KerasTensor: shape=(None, 128) dtype=float32 (created by layer 'lstm')>, <KerasTensor: shape=(None, 128) dtype=float32 (created by layer 'lstm')>, <KerasTensor: shape=(None, 128) dtype=float32 (created by layer 'lstm')>]\n"]}],"source":["model.summary()\n","# print(model.layers[-1].input)\n","print(model.layers[-3].output)"]},{"cell_type":"code","execution_count":9,"metadata":{"_uuid":"9f2c734844e2ba10ba58179226438888c0449d2d","trusted":true},"outputs":[{"name":"stdout","output_type":"stream","text":["encoder_input_data shape: (8000, 182, 76)\n","decoder_input_data shape: (8000, 21, 38)\n","decoder_target_data shape: (8000, 21, 38)\n"]}],"source":["print(\"encoder_input_data shape:\",encoder_input_data.shape)\n","print(\"decoder_input_data shape:\",decoder_input_data.shape)\n","print(\"decoder_target_data shape:\",decoder_target_data.shape)"]},{"cell_type":"code","execution_count":10,"metadata":{"_uuid":"4fb27c5a375ac72ecbc648404d849b9ddd56dfc0","trusted":true},"outputs":[{"name":"stdout","output_type":"stream","text":["You must install pydot (`pip install pydot`) and install graphviz (see instructions at https://graphviz.gitlab.io/download/) for plot_model/model_to_dot to work.\n"]}],"source":["plot_model(model,show_shapes=True)"]},{"cell_type":"code","execution_count":11,"metadata":{"_uuid":"bcd0a4bd8b4cd2431539c7c507154f5eed78422f","trusted":true},"outputs":[{"name":"stderr","output_type":"stream","text":["c:\\Users\\humbe\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\keras\\optimizers\\optimizer_v2\\adam.py:110: UserWarning: The `lr` argument is deprecated, use `learning_rate` instead.\n","  super(Adam, self).__init__(name, **kwargs)\n"]},{"name":"stdout","output_type":"stream","text":["Epoch 1/100\n","100/100 [==============================] - 6s 31ms/step - loss: 0.6742 - val_loss: 0.6890\n","Epoch 2/100\n","100/100 [==============================] - 2s 24ms/step - loss: 0.6324 - val_loss: 0.6829\n","Epoch 3/100\n","100/100 [==============================] - 2s 24ms/step - loss: 0.6273 - val_loss: 0.6963\n","Epoch 4/100\n","100/100 [==============================] - 2s 23ms/step - loss: 0.6852 - val_loss: 0.6876\n","Epoch 5/100\n","100/100 [==============================] - 2s 24ms/step - loss: 0.6241 - val_loss: 0.6824\n","Epoch 6/100\n","100/100 [==============================] - 2s 24ms/step - loss: 0.6154 - val_loss: 0.6752\n","Epoch 7/100\n","100/100 [==============================] - 2s 23ms/step - loss: 0.6112 - val_loss: 0.6743\n","Epoch 8/100\n","100/100 [==============================] - 2s 23ms/step - loss: 0.6083 - val_loss: 0.6720\n","Epoch 9/100\n","100/100 [==============================] - 2s 23ms/step - loss: 0.6065 - val_loss: 0.6692\n","Epoch 10/100\n","100/100 [==============================] - 2s 24ms/step - loss: 0.6049 - val_loss: 0.6713\n","Epoch 11/100\n","100/100 [==============================] - 2s 24ms/step - loss: 0.6042 - val_loss: 0.6686\n","Epoch 12/100\n","100/100 [==============================] - 2s 24ms/step - loss: 0.6033 - val_loss: 0.6679\n","Epoch 13/100\n","100/100 [==============================] - 2s 23ms/step - loss: 0.6027 - val_loss: 0.6777\n","Epoch 14/100\n","100/100 [==============================] - 2s 23ms/step - loss: 0.6082 - val_loss: 0.6711\n","Epoch 15/100\n","100/100 [==============================] - 2s 23ms/step - loss: 0.6071 - val_loss: 0.6689\n","Epoch 16/100\n","100/100 [==============================] - 2s 23ms/step - loss: 0.6064 - val_loss: 0.6693\n","Epoch 17/100\n","100/100 [==============================] - 2s 23ms/step - loss: 0.6054 - val_loss: 0.6681\n","Epoch 18/100\n","100/100 [==============================] - 2s 23ms/step - loss: 0.6062 - val_loss: 0.6678\n","Epoch 19/100\n","100/100 [==============================] - 2s 23ms/step - loss: 0.6084 - val_loss: 0.6650\n","Epoch 20/100\n","100/100 [==============================] - 2s 23ms/step - loss: 0.6073 - val_loss: 0.6654\n","Epoch 21/100\n","100/100 [==============================] - 2s 23ms/step - loss: 0.6071 - val_loss: 0.6653\n","Epoch 22/100\n","100/100 [==============================] - 2s 24ms/step - loss: 0.6068 - val_loss: 0.6654\n","Epoch 23/100\n","100/100 [==============================] - 2s 23ms/step - loss: 0.6065 - val_loss: 0.6652\n","Epoch 24/100\n","100/100 [==============================] - 2s 23ms/step - loss: 0.6062 - val_loss: 0.6654\n","Epoch 25/100\n","100/100 [==============================] - 2s 23ms/step - loss: 0.6060 - val_loss: 0.6644\n","Epoch 26/100\n","100/100 [==============================] - 2s 23ms/step - loss: 0.6056 - val_loss: 0.6638\n","Epoch 27/100\n","100/100 [==============================] - 2s 23ms/step - loss: 0.6054 - val_loss: 0.6648\n","Epoch 28/100\n","100/100 [==============================] - 2s 23ms/step - loss: 0.6054 - val_loss: 0.6649\n","Epoch 29/100\n","100/100 [==============================] - 2s 23ms/step - loss: 0.6055 - val_loss: 0.6650\n","Epoch 30/100\n","100/100 [==============================] - 2s 23ms/step - loss: 0.6054 - val_loss: 0.6667\n","Epoch 31/100\n","100/100 [==============================] - 2s 23ms/step - loss: 0.6048 - val_loss: 0.6664\n","Epoch 32/100\n","100/100 [==============================] - 2s 23ms/step - loss: 0.6044 - val_loss: 0.6657\n","Epoch 33/100\n","100/100 [==============================] - 2s 23ms/step - loss: 0.6046 - val_loss: 0.6649\n","Epoch 34/100\n","100/100 [==============================] - 2s 23ms/step - loss: 0.6044 - val_loss: 0.6650\n","Epoch 35/100\n","100/100 [==============================] - 2s 23ms/step - loss: 0.6045 - val_loss: 0.6658\n","Epoch 36/100\n","100/100 [==============================] - 2s 23ms/step - loss: 0.6040 - val_loss: 0.6641\n","Epoch 37/100\n","100/100 [==============================] - 2s 23ms/step - loss: 0.6036 - val_loss: 0.6667\n","Epoch 38/100\n","100/100 [==============================] - 2s 23ms/step - loss: 0.6037 - val_loss: 0.6663\n","Epoch 39/100\n","100/100 [==============================] - 2s 23ms/step - loss: 0.6036 - val_loss: 0.6639\n","Epoch 40/100\n","100/100 [==============================] - 2s 23ms/step - loss: 0.6160 - val_loss: 0.6957\n","Epoch 41/100\n","100/100 [==============================] - 2s 23ms/step - loss: 0.6282 - val_loss: 0.6914\n","Epoch 42/100\n","100/100 [==============================] - 2s 23ms/step - loss: 0.6273 - val_loss: 0.6907\n","Epoch 43/100\n","100/100 [==============================] - 2s 23ms/step - loss: 0.6267 - val_loss: 0.6916\n","Epoch 44/100\n","100/100 [==============================] - 2s 23ms/step - loss: 0.6262 - val_loss: 0.6901\n","Epoch 45/100\n","100/100 [==============================] - 2s 23ms/step - loss: 0.6254 - val_loss: 0.6891\n","Epoch 46/100\n","100/100 [==============================] - 2s 23ms/step - loss: 0.6242 - val_loss: 0.6898\n","Epoch 47/100\n","100/100 [==============================] - 2s 23ms/step - loss: 0.6237 - val_loss: 0.6896\n","Epoch 48/100\n","100/100 [==============================] - 2s 23ms/step - loss: 0.6231 - val_loss: 0.6880\n","Epoch 49/100\n","100/100 [==============================] - 2s 23ms/step - loss: 0.6226 - val_loss: 0.6879\n","Epoch 50/100\n","100/100 [==============================] - 2s 24ms/step - loss: 0.6219 - val_loss: 0.6892\n","Epoch 51/100\n","100/100 [==============================] - 2s 23ms/step - loss: 0.6217 - val_loss: 0.6884\n","Epoch 52/100\n","100/100 [==============================] - 2s 23ms/step - loss: 0.6214 - val_loss: 0.6883\n","Epoch 53/100\n","100/100 [==============================] - 2s 23ms/step - loss: 0.6204 - val_loss: 0.6854\n","Epoch 54/100\n","100/100 [==============================] - 2s 23ms/step - loss: 0.6191 - val_loss: 0.6861\n","Epoch 55/100\n","100/100 [==============================] - 2s 23ms/step - loss: 0.6186 - val_loss: 0.6843\n","Epoch 56/100\n","100/100 [==============================] - 2s 24ms/step - loss: 0.6183 - val_loss: 0.6833\n","Epoch 57/100\n","100/100 [==============================] - 2s 23ms/step - loss: 0.6177 - val_loss: 0.6833\n","Epoch 58/100\n","100/100 [==============================] - 2s 23ms/step - loss: 0.6177 - val_loss: 0.6829\n","Epoch 59/100\n","100/100 [==============================] - 2s 23ms/step - loss: 0.6173 - val_loss: 0.6833\n","Epoch 60/100\n","100/100 [==============================] - 2s 23ms/step - loss: 0.6173 - val_loss: 0.6835\n","Epoch 61/100\n","100/100 [==============================] - 2s 23ms/step - loss: 0.6168 - val_loss: 0.6832\n","Epoch 62/100\n","100/100 [==============================] - 2s 23ms/step - loss: 0.6165 - val_loss: 0.6828\n","Epoch 63/100\n","100/100 [==============================] - 2s 23ms/step - loss: 0.6158 - val_loss: 0.6826\n","Epoch 64/100\n","100/100 [==============================] - 2s 23ms/step - loss: 0.6156 - val_loss: 0.6823\n","Epoch 65/100\n","100/100 [==============================] - 2s 23ms/step - loss: 0.6149 - val_loss: 0.6816\n","Epoch 66/100\n","100/100 [==============================] - 2s 23ms/step - loss: 0.6145 - val_loss: 0.6807\n","Epoch 67/100\n","100/100 [==============================] - 2s 23ms/step - loss: 0.6138 - val_loss: 0.6809\n","Epoch 68/100\n","100/100 [==============================] - 2s 23ms/step - loss: 0.6137 - val_loss: 0.6786\n","Epoch 69/100\n","100/100 [==============================] - 2s 23ms/step - loss: 0.6133 - val_loss: 0.6810\n","Epoch 70/100\n","100/100 [==============================] - 2s 23ms/step - loss: 0.6134 - val_loss: 0.6805\n","Epoch 71/100\n","100/100 [==============================] - 2s 23ms/step - loss: 0.6134 - val_loss: 0.6782\n","Epoch 72/100\n","100/100 [==============================] - 2s 23ms/step - loss: 0.6128 - val_loss: 0.6799\n","Epoch 73/100\n","100/100 [==============================] - 2s 23ms/step - loss: 0.6127 - val_loss: 0.6779\n","Epoch 74/100\n","100/100 [==============================] - 2s 23ms/step - loss: 0.6125 - val_loss: 0.6786\n","Epoch 75/100\n","100/100 [==============================] - 2s 23ms/step - loss: 0.6121 - val_loss: 0.6793\n","Epoch 76/100\n","100/100 [==============================] - 2s 23ms/step - loss: 0.6122 - val_loss: 0.6776\n","Epoch 77/100\n","100/100 [==============================] - 2s 23ms/step - loss: 0.6116 - val_loss: 0.6790\n","Epoch 78/100\n","100/100 [==============================] - 2s 23ms/step - loss: 0.6119 - val_loss: 0.6787\n","Epoch 79/100\n","100/100 [==============================] - 2s 23ms/step - loss: 0.6115 - val_loss: 0.6783\n","Epoch 80/100\n","100/100 [==============================] - 2s 23ms/step - loss: 0.6114 - val_loss: 0.6788\n","Epoch 81/100\n","100/100 [==============================] - 2s 23ms/step - loss: 0.6113 - val_loss: 0.6791\n","Epoch 82/100\n","100/100 [==============================] - 2s 23ms/step - loss: 0.6113 - val_loss: 0.6773\n","Epoch 83/100\n","100/100 [==============================] - 2s 23ms/step - loss: 0.6110 - val_loss: 0.6785\n","Epoch 84/100\n","100/100 [==============================] - 2s 23ms/step - loss: 0.6110 - val_loss: 0.6779\n","Epoch 85/100\n","100/100 [==============================] - 2s 23ms/step - loss: 0.6107 - val_loss: 0.6776\n","Epoch 86/100\n","100/100 [==============================] - 2s 24ms/step - loss: 0.6108 - val_loss: 0.6780\n","Epoch 87/100\n","100/100 [==============================] - 2s 23ms/step - loss: 0.6106 - val_loss: 0.6782\n","Epoch 88/100\n","100/100 [==============================] - 2s 23ms/step - loss: 0.6106 - val_loss: 0.6773\n","Epoch 89/100\n","100/100 [==============================] - 2s 23ms/step - loss: 0.6104 - val_loss: 0.6778\n","Epoch 90/100\n","100/100 [==============================] - 2s 24ms/step - loss: 0.6104 - val_loss: 0.6778\n","Epoch 91/100\n","100/100 [==============================] - 2s 23ms/step - loss: 0.6103 - val_loss: 0.6772\n","Epoch 92/100\n","100/100 [==============================] - 2s 23ms/step - loss: 0.6102 - val_loss: 0.6776\n","Epoch 93/100\n","100/100 [==============================] - 2s 23ms/step - loss: 0.6099 - val_loss: 0.6779\n","Epoch 94/100\n","100/100 [==============================] - 2s 23ms/step - loss: 0.6099 - val_loss: 0.6771\n","Epoch 95/100\n","100/100 [==============================] - 2s 23ms/step - loss: 0.6099 - val_loss: 0.6771\n","Epoch 96/100\n","100/100 [==============================] - 2s 23ms/step - loss: 0.6098 - val_loss: 0.6772\n","Epoch 97/100\n","100/100 [==============================] - 2s 24ms/step - loss: 0.6094 - val_loss: 0.6779\n","Epoch 98/100\n","100/100 [==============================] - 2s 23ms/step - loss: 0.6093 - val_loss: 0.6775\n","Epoch 99/100\n","100/100 [==============================] - 2s 23ms/step - loss: 0.6090 - val_loss: 0.6782\n","Epoch 100/100\n","100/100 [==============================] - 2s 23ms/step - loss: 0.6087 - val_loss: 0.6779\n"]},{"data":{"text/plain":["<keras.callbacks.History at 0x1c421864820>"]},"execution_count":11,"metadata":{},"output_type":"execute_result"}],"source":["\n","# Run training\n","from keras.optimizers import * \n","model.compile(optimizer=Adam(lr=0.01, beta_1=0.9, beta_2=0.999, decay=0.001), loss='categorical_crossentropy')\n","model.fit([encoder_input_data, decoder_input_data], decoder_target_data,\n","          batch_size=batch_size,\n","          epochs=epochs, \n","          validation_split=0.2)"]},{"cell_type":"code","execution_count":12,"metadata":{"_uuid":"fd0f32d41296a9cccc9639092c591582448abebb","trusted":true},"outputs":[],"source":["# Define sampling models\n","encoder_model = Model(encoder_inputs, encoder_states)\n","decoder_state_input_h = Input(shape=(latent_dim,))\n","decoder_state_input_c = Input(shape=(latent_dim,))\n","decoder_states_inputs = [decoder_state_input_h, decoder_state_input_c]\n","decoder_outputs, state_h, state_c = decoder_lstm(\n","    decoder_inputs, initial_state=decoder_states_inputs)\n","decoder_states = [state_h, state_c]\n","decoder_outputs = decoder_dense(decoder_outputs)\n","decoder_model = Model(\n","    [decoder_inputs] + decoder_states_inputs,\n","    [decoder_outputs] + decoder_states)"]},{"cell_type":"code","execution_count":13,"metadata":{"_uuid":"56264615ed9b13fe15c0c4ea0d53a138625b0253","trusted":true},"outputs":[],"source":["# Reverse-lookup token index to decode sequences back to\n","# something readable.\n","reverse_input_char_index = dict(\n","    (i, char) for char, i in input_token_index.items())\n","reverse_target_char_index = dict(\n","    (i, char) for char, i in target_token_index.items())\n","\n","\n","def decode_sequence(input_seq):\n","    # Encode the input as state vectors.\n","    states_value = encoder_model.predict(input_seq)\n","\n","    # Generate empty target sequence of length 1.\n","    target_seq = np.zeros((1, 1, num_decoder_tokens))\n","    # Populate the first character of target sequence with the start character.\n","    target_seq[0, 0, target_token_index['\\t']] = 1.\n","\n","    # Sampling loop for a batch of sequences\n","    # (to simplify, here we assume a batch of size 1).\n","    stop_condition = False\n","    decoded_sentence = ''\n","    while not stop_condition:\n","        output_tokens, h, c = decoder_model.predict(\n","            [target_seq] + states_value)\n","\n","        # Sample a token\n","        sampled_token_index = np.argmax(output_tokens[0, -1, :])\n","        sampled_char = reverse_target_char_index[sampled_token_index]\n","        decoded_sentence += sampled_char\n","\n","        # Exit condition: either hit max length\n","        # or find stop character.\n","        if (sampled_char == '\\n' or\n","           len(decoded_sentence) > max_decoder_seq_length):\n","            stop_condition = True\n","\n","        # Update the target sequence (of length 1).\n","        target_seq = np.zeros((1, 1, num_decoder_tokens))\n","        target_seq[0, 0, sampled_token_index] = 1.\n","\n","        # Update states\n","        states_value = [h, c]\n","\n","    return decoded_sentence"]},{"cell_type":"code","execution_count":14,"metadata":{"_uuid":"be16fcb708fb9a89ae7b4f447de86ec0a7654db6","trusted":true},"outputs":[{"name":"stdout","output_type":"stream","text":["1/1 [==============================] - 0s 312ms/step\n","1/1 [==============================] - 0s 293ms/step\n","1/1 [==============================] - 0s 18ms/step\n","1/1 [==============================] - 0s 17ms/step\n","1/1 [==============================] - 0s 17ms/step\n","1/1 [==============================] - 0s 16ms/step\n","1/1 [==============================] - 0s 17ms/step\n","1/1 [==============================] - 0s 16ms/step\n","1/1 [==============================] - 0s 16ms/step\n","1/1 [==============================] - 0s 17ms/step\n","1/1 [==============================] - 0s 16ms/step\n","1/1 [==============================] - 0s 17ms/step\n","1/1 [==============================] - 0s 20ms/step\n","1/1 [==============================] - 0s 16ms/step\n","1/1 [==============================] - 0s 17ms/step\n","1/1 [==============================] - 0s 17ms/step\n","1/1 [==============================] - 0s 17ms/step\n","1/1 [==============================] - 0s 17ms/step\n","1/1 [==============================] - 0s 17ms/step\n","1/1 [==============================] - 0s 17ms/step\n","1/1 [==============================] - 0s 18ms/step\n","1/1 [==============================] - 0s 17ms/step\n","1/1 [==============================] - 0s 17ms/step\n","-\n","Input sentence: title\n","Decoded sentence: 1.0...................\n","1/1 [==============================] - 0s 17ms/step\n","1/1 [==============================] - 0s 16ms/step\n","1/1 [==============================] - 0s 17ms/step\n","1/1 [==============================] - 0s 17ms/step\n","1/1 [==============================] - 0s 16ms/step\n","1/1 [==============================] - 0s 16ms/step\n","1/1 [==============================] - 0s 18ms/step\n","1/1 [==============================] - 0s 16ms/step\n","1/1 [==============================] - 0s 17ms/step\n","1/1 [==============================] - 0s 17ms/step\n","1/1 [==============================] - 0s 17ms/step\n","1/1 [==============================] - 0s 17ms/step\n","1/1 [==============================] - 0s 18ms/step\n","1/1 [==============================] - 0s 17ms/step\n","1/1 [==============================] - 0s 17ms/step\n","1/1 [==============================] - 0s 17ms/step\n","1/1 [==============================] - 0s 18ms/step\n","1/1 [==============================] - 0s 17ms/step\n","1/1 [==============================] - 0s 17ms/step\n","1/1 [==============================] - 0s 17ms/step\n","1/1 [==============================] - 0s 17ms/step\n","1/1 [==============================] - 0s 16ms/step\n","1/1 [==============================] - 0s 18ms/step\n","-\n","Input sentence: 01org Tpm2.0-tools 1.1.0\n","Decoded sentence: 1.0...................\n","1/1 [==============================] - 0s 17ms/step\n","1/1 [==============================] - 0s 18ms/step\n","1/1 [==============================] - 0s 17ms/step\n","1/1 [==============================] - 0s 17ms/step\n","1/1 [==============================] - 0s 18ms/step\n","1/1 [==============================] - 0s 17ms/step\n","1/1 [==============================] - 0s 19ms/step\n","1/1 [==============================] - 0s 17ms/step\n","1/1 [==============================] - 0s 18ms/step\n","1/1 [==============================] - 0s 20ms/step\n","1/1 [==============================] - 0s 17ms/step\n","1/1 [==============================] - 0s 17ms/step\n","1/1 [==============================] - 0s 19ms/step\n","1/1 [==============================] - 0s 18ms/step\n","1/1 [==============================] - 0s 18ms/step\n","1/1 [==============================] - 0s 17ms/step\n","1/1 [==============================] - 0s 17ms/step\n","1/1 [==============================] - 0s 17ms/step\n","1/1 [==============================] - 0s 18ms/step\n","1/1 [==============================] - 0s 18ms/step\n","1/1 [==============================] - 0s 20ms/step\n","1/1 [==============================] - 0s 17ms/step\n","1/1 [==============================] - 0s 18ms/step\n","-\n","Input sentence: 0xacab mat2\n","Decoded sentence: 1.0...................\n","1/1 [==============================] - 0s 17ms/step\n","1/1 [==============================] - 0s 17ms/step\n","1/1 [==============================] - 0s 17ms/step\n","1/1 [==============================] - 0s 17ms/step\n","1/1 [==============================] - 0s 17ms/step\n","1/1 [==============================] - 0s 17ms/step\n","1/1 [==============================] - 0s 18ms/step\n","1/1 [==============================] - 0s 18ms/step\n","1/1 [==============================] - 0s 17ms/step\n","1/1 [==============================] - 0s 17ms/step\n","1/1 [==============================] - 0s 17ms/step\n","1/1 [==============================] - 0s 17ms/step\n","1/1 [==============================] - 0s 17ms/step\n","1/1 [==============================] - 0s 17ms/step\n","1/1 [==============================] - 0s 17ms/step\n","1/1 [==============================] - 0s 18ms/step\n","1/1 [==============================] - 0s 17ms/step\n","1/1 [==============================] - 0s 17ms/step\n","1/1 [==============================] - 0s 18ms/step\n","1/1 [==============================] - 0s 17ms/step\n","1/1 [==============================] - 0s 18ms/step\n","1/1 [==============================] - 0s 16ms/step\n","1/1 [==============================] - 0s 18ms/step\n","-\n","Input sentence: 0xacab mat2 0.1.0\n","Decoded sentence: 1.0...................\n","1/1 [==============================] - 0s 20ms/step\n","1/1 [==============================] - 0s 18ms/step\n","1/1 [==============================] - 0s 16ms/step\n","1/1 [==============================] - 0s 17ms/step\n","1/1 [==============================] - 0s 17ms/step\n","1/1 [==============================] - 0s 17ms/step\n","1/1 [==============================] - 0s 17ms/step\n","1/1 [==============================] - 0s 17ms/step\n","1/1 [==============================] - 0s 19ms/step\n","1/1 [==============================] - 0s 17ms/step\n","1/1 [==============================] - 0s 16ms/step\n","1/1 [==============================] - 0s 18ms/step\n","1/1 [==============================] - 0s 17ms/step\n","1/1 [==============================] - 0s 18ms/step\n","1/1 [==============================] - 0s 20ms/step\n","1/1 [==============================] - 0s 17ms/step\n","1/1 [==============================] - 0s 18ms/step\n","1/1 [==============================] - 0s 18ms/step\n","1/1 [==============================] - 0s 19ms/step\n","1/1 [==============================] - 0s 18ms/step\n","1/1 [==============================] - 0s 18ms/step\n","1/1 [==============================] - 0s 20ms/step\n","1/1 [==============================] - 0s 18ms/step\n","-\n","Input sentence: 0xacab mat2 0.1.1\n","Decoded sentence: 1.0...................\n","1/1 [==============================] - 0s 16ms/step\n","1/1 [==============================] - 0s 20ms/step\n","1/1 [==============================] - 0s 18ms/step\n","1/1 [==============================] - 0s 18ms/step\n","1/1 [==============================] - 0s 19ms/step\n","1/1 [==============================] - 0s 18ms/step\n","1/1 [==============================] - 0s 18ms/step\n","1/1 [==============================] - 0s 17ms/step\n","1/1 [==============================] - 0s 18ms/step\n","1/1 [==============================] - 0s 18ms/step\n","1/1 [==============================] - 0s 18ms/step\n","1/1 [==============================] - 0s 18ms/step\n","1/1 [==============================] - 0s 18ms/step\n","1/1 [==============================] - 0s 20ms/step\n","1/1 [==============================] - 0s 19ms/step\n","1/1 [==============================] - 0s 18ms/step\n","1/1 [==============================] - 0s 17ms/step\n","1/1 [==============================] - 0s 19ms/step\n","1/1 [==============================] - 0s 18ms/step\n","1/1 [==============================] - 0s 18ms/step\n","1/1 [==============================] - 0s 18ms/step\n","1/1 [==============================] - 0s 19ms/step\n","1/1 [==============================] - 0s 18ms/step\n","-\n","Input sentence: 0xacab mat2 0.1.2\n","Decoded sentence: 1.0...................\n","1/1 [==============================] - 0s 17ms/step\n","1/1 [==============================] - 0s 20ms/step\n","1/1 [==============================] - 0s 18ms/step\n","1/1 [==============================] - 0s 17ms/step\n","1/1 [==============================] - 0s 21ms/step\n","1/1 [==============================] - 0s 18ms/step\n","1/1 [==============================] - 0s 19ms/step\n","1/1 [==============================] - 0s 17ms/step\n","1/1 [==============================] - 0s 18ms/step\n","1/1 [==============================] - 0s 19ms/step\n","1/1 [==============================] - 0s 18ms/step\n","1/1 [==============================] - 0s 18ms/step\n","1/1 [==============================] - 0s 17ms/step\n","1/1 [==============================] - 0s 18ms/step\n","1/1 [==============================] - 0s 20ms/step\n","1/1 [==============================] - 0s 18ms/step\n","1/1 [==============================] - 0s 18ms/step\n","1/1 [==============================] - 0s 18ms/step\n","1/1 [==============================] - 0s 18ms/step\n","1/1 [==============================] - 0s 18ms/step\n","1/1 [==============================] - 0s 18ms/step\n","1/1 [==============================] - 0s 17ms/step\n","1/1 [==============================] - 0s 18ms/step\n","-\n","Input sentence: 0xacab mat2 0.1.3\n","Decoded sentence: 1.0...................\n","1/1 [==============================] - 0s 17ms/step\n","1/1 [==============================] - 0s 18ms/step\n","1/1 [==============================] - 0s 17ms/step\n","1/1 [==============================] - 0s 20ms/step\n","1/1 [==============================] - 0s 18ms/step\n","1/1 [==============================] - 0s 18ms/step\n","1/1 [==============================] - 0s 18ms/step\n","1/1 [==============================] - 0s 17ms/step\n","1/1 [==============================] - 0s 18ms/step\n","1/1 [==============================] - 0s 18ms/step\n","1/1 [==============================] - 0s 18ms/step\n","1/1 [==============================] - 0s 18ms/step\n","1/1 [==============================] - 0s 17ms/step\n","1/1 [==============================] - 0s 20ms/step\n","1/1 [==============================] - 0s 18ms/step\n","1/1 [==============================] - 0s 18ms/step\n","1/1 [==============================] - 0s 18ms/step\n","1/1 [==============================] - 0s 17ms/step\n","1/1 [==============================] - 0s 18ms/step\n","1/1 [==============================] - 0s 19ms/step\n","1/1 [==============================] - 0s 19ms/step\n","1/1 [==============================] - 0s 17ms/step\n","1/1 [==============================] - 0s 17ms/step\n","-\n","Input sentence: 0xacab mat2 0.2.0\n","Decoded sentence: 1.0...................\n","1/1 [==============================] - 0s 20ms/step\n","1/1 [==============================] - 0s 18ms/step\n","1/1 [==============================] - 0s 21ms/step\n","1/1 [==============================] - 0s 18ms/step\n","1/1 [==============================] - 0s 18ms/step\n","1/1 [==============================] - 0s 18ms/step\n","1/1 [==============================] - 0s 18ms/step\n","1/1 [==============================] - 0s 17ms/step\n","1/1 [==============================] - 0s 17ms/step\n","1/1 [==============================] - 0s 17ms/step\n","1/1 [==============================] - 0s 19ms/step\n","1/1 [==============================] - 0s 17ms/step\n","1/1 [==============================] - 0s 18ms/step\n","1/1 [==============================] - 0s 17ms/step\n","1/1 [==============================] - 0s 17ms/step\n","1/1 [==============================] - 0s 19ms/step\n","1/1 [==============================] - 0s 18ms/step\n","1/1 [==============================] - 0s 19ms/step\n","1/1 [==============================] - 0s 18ms/step\n","1/1 [==============================] - 0s 18ms/step\n","1/1 [==============================] - 0s 18ms/step\n","1/1 [==============================] - 0s 18ms/step\n","1/1 [==============================] - 0s 18ms/step\n","-\n","Input sentence: 0xacab mat2 0.3.0\n","Decoded sentence: 1.0...................\n","1/1 [==============================] - 0s 19ms/step\n","1/1 [==============================] - 0s 19ms/step\n","1/1 [==============================] - 0s 17ms/step\n","1/1 [==============================] - 0s 18ms/step\n","1/1 [==============================] - 0s 18ms/step\n","1/1 [==============================] - 0s 18ms/step\n","1/1 [==============================] - 0s 18ms/step\n","1/1 [==============================] - 0s 20ms/step\n","1/1 [==============================] - 0s 19ms/step\n","1/1 [==============================] - 0s 18ms/step\n","1/1 [==============================] - 0s 18ms/step\n","1/1 [==============================] - 0s 19ms/step\n","1/1 [==============================] - 0s 18ms/step\n","1/1 [==============================] - 0s 17ms/step\n","1/1 [==============================] - 0s 18ms/step\n","1/1 [==============================] - 0s 17ms/step\n","1/1 [==============================] - 0s 17ms/step\n","1/1 [==============================] - 0s 18ms/step\n","1/1 [==============================] - 0s 19ms/step\n","1/1 [==============================] - 0s 17ms/step\n","1/1 [==============================] - 0s 18ms/step\n","1/1 [==============================] - 0s 17ms/step\n","1/1 [==============================] - 0s 18ms/step\n","-\n","Input sentence: 0xacab mat2 0.3.1\n","Decoded sentence: 1.0...................\n"]}],"source":["\n","for seq_index in range(10):\n","    # Take one sequence (part of the training set)\n","    # for trying out decoding.\n","    input_seq = encoder_input_data[seq_index: seq_index + 1]\n","    decoded_sentence = decode_sequence(input_seq)\n","    print('-')\n","    print('Input sentence:', input_texts[seq_index])\n","    print('Decoded sentence:', decoded_sentence)"]},{"cell_type":"code","execution_count":null,"metadata":{"_uuid":"499e7f2cde6d03cc871b0a1ec372498827f731a3","trusted":true},"outputs":[],"source":[]},{"cell_type":"code","execution_count":null,"metadata":{"_uuid":"b511cb4777fa0113377a87582d366541ea949a10","trusted":true},"outputs":[],"source":[]},{"cell_type":"code","execution_count":null,"metadata":{"_uuid":"5f19fc14dd5cc64ad0c81c33a354dbed43f7236a","trusted":true},"outputs":[],"source":[]}],"metadata":{"kernelspec":{"display_name":"Python 3.10.6 64-bit","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.10.6"},"vscode":{"interpreter":{"hash":"5264f721a23242131e2cd1435c6f396c4a07f3216f42f5a927698946e7ff8eef"}}},"nbformat":4,"nbformat_minor":1}
