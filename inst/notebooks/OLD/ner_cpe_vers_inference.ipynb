{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Required packages\n",
    "import torch\n",
    "import re\n",
    "import csv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[0.5268, 0.7979, 0.9768],\n",
      "        [0.2683, 0.2382, 0.5579],\n",
      "        [0.0706, 0.3281, 0.6826],\n",
      "        [0.8678, 0.5696, 0.0609],\n",
      "        [0.2655, 0.9106, 0.5374]])\n"
     ]
    }
   ],
   "source": [
    "# Check if GPU is configured\n",
    "print(torch.rand(5, 3))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_path = \"models/ner_rasa_vers_v3\"\n",
    "test_path = \"/DEVEL/code/data/inventory.csv\"\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Load Inventory data set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "raw_text = \"\"\n",
    "with open(test_path) as csvfile:\n",
    "    raw_csv = csv.reader(csvfile, delimiter=',')\n",
    "    for row in raw_csv:\n",
    "        raw_text = raw_text + \"\\n\" + row[1]\n",
    "\n",
    "# remove last empty line\n",
    "input_text = raw_text.split(\"\\n\")[1:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def get_tokens_with_entities(raw_text: str):\n",
    "    # split the text by spaces only if the space does not occur between square brackets\n",
    "    # we do not want to split \"multi-word\" entity value yet\n",
    "    raw_tokens = re.split(r\"\\s(?![^\\[]*\\])\", raw_text)\n",
    "\n",
    "    # a regex for matching the annotation according to our notation [entity_value](entity_name)\n",
    "    entity_value_pattern = r\"\\[(?P<value>.+?)\\]\\((?P<entity>.+?)\\)\"\n",
    "    entity_value_pattern_compiled = re.compile(entity_value_pattern, flags=re.I|re.M)\n",
    "\n",
    "    tokens_with_entities = []\n",
    "\n",
    "    for raw_token in raw_tokens:\n",
    "        match = entity_value_pattern_compiled.match(raw_token)\n",
    "        if match:\n",
    "            raw_entity_name, raw_entity_value = match.group(\"entity\"), match.group(\"value\")\n",
    "\n",
    "            # we prefix the name of entity differently\n",
    "            # B- indicates beginning of an entity\n",
    "            # I- indicates the token is not a new entity itself but rather a part of existing one\n",
    "            for i, raw_entity_token in enumerate(re.split(\"\\s\", raw_entity_value)):\n",
    "                entity_prefix = \"B\" if i == 0 else \"I\"\n",
    "                entity_name = f\"{entity_prefix}-{raw_entity_name}\"\n",
    "                tokens_with_entities.append((raw_entity_token, entity_name))\n",
    "        else:\n",
    "            tokens_with_entities.append((raw_token, \"O\"))\n",
    "\n",
    "    return tokens_with_entities"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('adobe', 'O'), ('acrobat', 'B-cpe_product'), ('x', 'O'), ('(10.1)', 'O')]\n",
      "[('red', 'O'), ('hat', 'O'), ('wildfly', 'B-cpe_product'), ('core', 'I-cpe_product'), ('2.0.0', 'O'), ('alpha', 'O'), ('2', 'O')]\n",
      "[('microsoft', 'O'), ('corporation', 'O'), ('visualÂ', 'O'), ('studio', 'O'), ('community', 'O'), ('2022', 'O')]\n",
      "[('finalwire', 'O'), ('ltd.', 'O'), ('aida64', 'O'), ('extreme', 'O'), ('v6.60', 'O')]\n"
     ]
    }
   ],
   "source": [
    "print(get_tokens_with_entities(\"adobe [acrobat](cpe_product) x (10.1)\"))\n",
    "print(get_tokens_with_entities(\"red hat [wildfly core](cpe_product) 2.0.0 alpha 2\"))\n",
    "print(get_tokens_with_entities(input_text[0]))\n",
    "print(get_tokens_with_entities(input_text[1]))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "class NERDataMaker:\n",
    "    def __init__(self, texts):\n",
    "        self.unique_entities = []\n",
    "        self.processed_texts = []\n",
    "\n",
    "        temp_processed_texts = []\n",
    "        for text in texts:\n",
    "            tokens_with_entities = get_tokens_with_entities(text)\n",
    "            for _, ent in tokens_with_entities:\n",
    "                if ent not in self.unique_entities:\n",
    "                    self.unique_entities.append(ent)\n",
    "            temp_processed_texts.append(tokens_with_entities)\n",
    "\n",
    "        self.unique_entities.sort(key=lambda ent: ent if ent != \"O\" else \"\")\n",
    "\n",
    "        for tokens_with_entities in temp_processed_texts:\n",
    "            self.processed_texts.append([(t, self.unique_entities.index(ent)) for t, ent in tokens_with_entities])\n",
    "\n",
    "    @property\n",
    "    def id2label(self):\n",
    "        return dict(enumerate(self.unique_entities))\n",
    "\n",
    "    @property\n",
    "    def label2id(self):\n",
    "        return {v:k for k, v in self.id2label.items()}\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.processed_texts)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        def _process_tokens_for_one_text(id, tokens_with_encoded_entities):\n",
    "            ner_tags = []\n",
    "            tokens = []\n",
    "            for t, ent in tokens_with_encoded_entities:\n",
    "                ner_tags.append(ent)\n",
    "                tokens.append(t)\n",
    "\n",
    "            return {\n",
    "                \"id\": id,\n",
    "                \"ner_tags\": ner_tags,\n",
    "                \"tokens\": tokens\n",
    "            }\n",
    "\n",
    "        tokens_with_encoded_entities = self.processed_texts[idx]\n",
    "        if isinstance(idx, int):\n",
    "            return _process_tokens_for_one_text(idx, tokens_with_encoded_entities)\n",
    "        else:\n",
    "            return [_process_tokens_for_one_text(i+idx.start, tee) for i, tee in enumerate(tokens_with_encoded_entities)]\n",
    "\n",
    "    def as_hf_dataset(self, tokenizer):\n",
    "        from datasets import Dataset, Features, Value, ClassLabel, Sequence\n",
    "        def tokenize_and_align_labels(examples):\n",
    "            tokenized_inputs = tokenizer(examples[\"tokens\"], truncation=True, is_split_into_words=True)\n",
    "\n",
    "            labels = []\n",
    "            for i, label in enumerate(examples[f\"ner_tags\"]):\n",
    "                word_ids = tokenized_inputs.word_ids(batch_index=i)  # Map tokens to their respective word.\n",
    "                previous_word_idx = None\n",
    "                label_ids = []\n",
    "                for word_idx in word_ids:  # Set the special tokens to -100.\n",
    "                    if word_idx is None:\n",
    "                        label_ids.append(-100)\n",
    "                    elif word_idx != previous_word_idx:  # Only label the first token of a given word.\n",
    "                        label_ids.append(label[word_idx])\n",
    "                    else:\n",
    "                        label_ids.append(-100)\n",
    "                    previous_word_idx = word_idx\n",
    "                labels.append(label_ids)\n",
    "\n",
    "            tokenized_inputs[\"labels\"] = labels\n",
    "            return tokenized_inputs\n",
    "\n",
    "        ids, ner_tags, tokens = [], [], []\n",
    "        for i, pt in enumerate(self.processed_texts):\n",
    "            ids.append(i)\n",
    "            pt_tokens,pt_tags = list(zip(*pt))\n",
    "            ner_tags.append(pt_tags)\n",
    "            tokens.append(pt_tokens)\n",
    "        data = {\n",
    "            \"id\": ids,\n",
    "            \"ner_tags\": ner_tags,\n",
    "            \"tokens\": tokens\n",
    "        }\n",
    "        features = Features({\n",
    "            \"tokens\": Sequence(Value(\"string\")),\n",
    "            \"ner_tags\": Sequence(ClassLabel(names=dm.unique_entities)),\n",
    "            \"id\": Value(\"int32\")\n",
    "        })\n",
    "        ds = Dataset.from_dict(data, features)\n",
    "        tokenized_ds = ds.map(tokenize_and_align_labels, batched=True)\n",
    "        return tokenized_ds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "total examples = 295\n",
      "[{'id': 0, 'ner_tags': [0, 0, 0, 0, 0, 0], 'tokens': ['microsoft', 'corporation', 'visualÂ', 'studio', 'community', '2022']}, {'id': 1, 'ner_tags': [0, 0, 0, 0, 0], 'tokens': ['finalwire', 'ltd.', 'aida64', 'extreme', 'v6.60']}, {'id': 2, 'ner_tags': [0, 0, 0, 0, 0], 'tokens': ['glarysoft', 'ltd', 'glary', 'utilities', '5.187']}]\n"
     ]
    }
   ],
   "source": [
    "# Create Training NER Data Object\n",
    "dm = NERDataMaker(input_text)\n",
    "print(f\"total examples = {len(dm)}\")\n",
    "print(dm[0:3])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoConfig, AutoTokenizer, AutoModelForTokenClassification\n",
    "# model_config = AutoConfig.from_pretrained(model_path)\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_path + \"/tokenizer\")\n",
    "# model = AutoModelForTokenClassification.from_pretrained(\"models/ner_rasa_vpv_v2/tokenizer\", num_labels=len(dm.unique_entities), id2label=dm.id2label, label2id=dm.label2id)\n",
    "model = AutoModelForTokenClassification.from_pretrained(model_path)\n",
    "# id_to_label = {v: str(k) for k, v in model.config.label2id.items()}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "324df10ab05546109fda73f11a32e0eb",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1 [00:00<?, ?ba/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "input_ds = dm.as_hf_dataset(tokenizer=tokenizer)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# INFERENCE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{'entity_group': 'cpe_version',\n",
       "  'score': 0.99995387,\n",
       "  'word': '1',\n",
       "  'start': 16,\n",
       "  'end': 17}]"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from transformers import pipeline\n",
    "pipe = pipeline(\"ner\", model=model, tokenizer=tokenizer, aggregation_strategy=\"simple\", device=0) # pass device=0 if using gpu\n",
    "pipe(\"\"\"softing uagates 1.73 for wordpress\"\"\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# pipe(\"\"\"Microsoft Visual C++ 2013 Redistributable (x64) - 12.0.30501\"\"\")\n",
    "# pipe(\"\"\"microsoft visual c++ 2013 redistributable (x64) - 12.0.30501\"\"\")\n",
    "\n",
    "# pipe(\"\"\"google chrome 32.0.1670.5\"\"\")\n",
    "# pipe(\"\"\"draw.io 2.6.3 for confluence\"\"\")\n",
    "\n",
    "\n",
    "# pipe(\"\"\"progress sitefinity 9.2\"\"\")\n",
    "# pipe(\"bitnami containers 7.30.1-debian-10-r40 for laravel\")\n",
    "\n",
    "# pipe(\"cool house technology ewelink 4.3.0 for android\")\n",
    "# pipe(\"fastball productions fastball 2.5.3 for joomla\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.9.12 ('ner_poc')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "27f67507cd8543ab18e83f8a3a6d5dfd26fb3b380c571e44157f04b507d239be"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
